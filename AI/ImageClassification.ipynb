{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixDI8opPLV1U","executionInfo":{"status":"ok","timestamp":1716952404439,"user_tz":-540,"elapsed":24363,"user":{"displayName":"install SON","userId":"08860272883489928887"}},"outputId":"0bb75b14-34ac-4df3-c351-87a662e4e586"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyMSsb-Dj6ab","executionInfo":{"status":"ok","timestamp":1716952404440,"user_tz":-540,"elapsed":10,"user":{"displayName":"install SON","userId":"08860272883489928887"}},"outputId":"a38ca56e-15c2-45cb-c7b5-6e70c7690c0e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIpXy2UxVsfm","executionInfo":{"status":"ok","timestamp":1716952485754,"user_tz":-540,"elapsed":466,"user":{"displayName":"install SON","userId":"08860272883489928887"}},"outputId":"5e6ba90c-ac5c-4379-dbe3-54de31bcacf0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mtest\u001b[0m/                           \u001b[01;34mtrain\u001b[0m/                          'Untitled1img (1).ipynb'\n"," test-20240528T231138Z-001.zip   train-20240528T231137Z-001.zip\n"]}]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","# Google Drive에 있는 ZIP 파일 경로와 압축 해제할 폴더 경로 설정\n","zip_file_path = 'test-20240528T231138Z-001.zip'  # ZIP 파일 경로\n","extract_to_path = './'  # 압축 해제할 폴더 경로\n","\n","# 폴더가 없으면 생성\n","if not os.path.exists(extract_to_path):\n","    os.makedirs(extract_to_path)\n","\n","# ZIP 파일 열기 및 압축 해제\n","try:\n","    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to_path)\n","    print(f\"압축 해제 완료: {extract_to_path}\")\n","except zipfile.BadZipFile:\n","    print(\"Error: The file is not a zip file or it is corrupted.\")\n","except FileNotFoundError:\n","    print(f\"Error: The file {zip_file_path} does not exist.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ra6amy_nZOX","executionInfo":{"status":"ok","timestamp":1716952481803,"user_tz":-540,"elapsed":1397,"user":{"displayName":"install SON","userId":"08860272883489928887"}},"outputId":"9024900b-e56f-4980-d795-e63d8019b9e8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["압축 해제 완료: ./\n"]}]},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","import pandas as pd\n","\n","# 경로 설정\n","train_dir = './train'\n","test_dir = './test'\n","\n","# 이미지 데이터 제너레이터 설정\n","train_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)  # 데이터 정규화 및 검증 데이터 분할\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(32, 32),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(32, 32),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","# 프리트레인드 EfficientNet 모델 불러오기\n","base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n","\n","# 모델 정의\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dense(512, activation='relu'),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# 모델 컴파일\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // validation_generator.batch_size,\n","    epochs=10\n",")\n","\n","# 성능 지표 계산 및 출력\n","val_preds = model.predict(validation_generator)\n","val_labels = validation_generator.classes\n","\n","# MSE 및 RMSE 계산\n","mse = mean_squared_error(val_labels, np.argmax(val_preds, axis=1))\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(val_labels, np.argmax(val_preds, axis=1))\n","\n","print(f'MSE: {mse}')\n","print(f'RMSE: {rmse}')\n","print(f'MAE: {mae}')\n","\n","# 모델 저장\n","model.save('efficientnet_image_classifier_model.keras', save_format='keras')\n","\n","# 모델 불러오기\n","model = load_model('efficientnet_image_classifier_model.keras')\n","\n","# 이미지 파일 목록 가져오기\n","file_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith('.jpg') or fname.endswith('.png')]\n","\n","# 데이터프레임 생성\n","test_df = pd.DataFrame({\n","    'filename': file_paths,\n","    'class': ['unknown'] * len(file_paths)  # 임의의 클래스 레이블\n","})\n","\n","# 이미지 데이터 제너레이터 설정\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","\n","# 데이터프레임에서 데이터 로드\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    x_col='filename',\n","    y_col=None,\n","    target_size=(32, 32),\n","    batch_size=1,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# test_generator에 데이터가 있는지 확인\n","if len(test_generator.filenames) == 0:\n","    print(\"Error: No images found in the test directory.\")\n","else:\n","    print(f\"Found {len(test_generator.filenames)} images in the test directory.\")\n","\n","# 테스트 이미지 예측\n","test_preds = model.predict(test_generator)\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","\n","# 클래스 인덱스를 클래스 이름으로 매핑\n","class_indices = {v: k for k, v in train_generator.class_indices.items()}\n","test_pred_class_labels = [class_indices[label] for label in test_pred_labels]\n","\n","# 결과 저장\n","results = pd.DataFrame({\n","    'filename': test_generator.filenames,\n","    'predicted_label': test_pred_class_labels\n","})\n","results.to_csv('classification_results.csv', index=False)"],"metadata":{"id":"cETqDp70X-K9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716942310596,"user_tz":-540,"elapsed":232125,"user":{"displayName":"Inho Son","userId":"16864266206823282486"}},"outputId":"39804d1b-c8bd-4625-84ad-7ac347d1c98a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2400 images belonging to 3 classes.\n","Found 600 images belonging to 3 classes.\n","Epoch 1/10\n","75/75 [==============================] - 55s 192ms/step - loss: 0.9949 - accuracy: 0.5079 - val_loss: 1.1096 - val_accuracy: 0.3351\n","Epoch 2/10\n","75/75 [==============================] - 13s 171ms/step - loss: 0.7582 - accuracy: 0.6746 - val_loss: 1.1507 - val_accuracy: 0.3333\n","Epoch 3/10\n","75/75 [==============================] - 13s 168ms/step - loss: 0.6631 - accuracy: 0.7262 - val_loss: 1.1802 - val_accuracy: 0.3316\n","Epoch 4/10\n","75/75 [==============================] - 11s 149ms/step - loss: 0.5662 - accuracy: 0.7688 - val_loss: 1.2628 - val_accuracy: 0.3403\n","Epoch 5/10\n","75/75 [==============================] - 10s 137ms/step - loss: 0.5035 - accuracy: 0.7975 - val_loss: 1.4714 - val_accuracy: 0.3368\n","Epoch 6/10\n","75/75 [==============================] - 13s 171ms/step - loss: 0.4792 - accuracy: 0.8092 - val_loss: 1.7041 - val_accuracy: 0.3611\n","Epoch 7/10\n","75/75 [==============================] - 12s 157ms/step - loss: 0.4314 - accuracy: 0.8342 - val_loss: 1.2052 - val_accuracy: 0.4340\n","Epoch 8/10\n","75/75 [==============================] - 12s 161ms/step - loss: 0.3861 - accuracy: 0.8521 - val_loss: 1.0649 - val_accuracy: 0.5278\n","Epoch 9/10\n","75/75 [==============================] - 11s 147ms/step - loss: 0.3422 - accuracy: 0.8683 - val_loss: 1.6237 - val_accuracy: 0.5312\n","Epoch 10/10\n","75/75 [==============================] - 12s 160ms/step - loss: 0.2883 - accuracy: 0.8892 - val_loss: 1.0200 - val_accuracy: 0.5625\n","19/19 [==============================] - 3s 97ms/step\n","MSE: 1.2\n","RMSE: 1.0954451150103321\n","MAE: 0.8233333333333334\n","Found 45 validated image filenames.\n","Found 45 images in the test directory.\n","45/45 [==============================] - 3s 18ms/step\n"]}]},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import ResNet152\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","import pandas as pd\n","\n","# 경로 설정\n","train_dir = './train'\n","test_dir = './test'\n","\n","# 이미지 데이터 제너레이터 설정\n","train_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)  # 데이터 정규화 및 검증 데이터 분할\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","# 프리트레인드 ResNet-152 모델 불러오기\n","base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# 모델 정의\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dense(512, activation='relu'),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# 모델 컴파일\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    validation_data=validation_generator,\n","    validation_steps=len(validation_generator),\n","    epochs=10\n",")\n","\n","# 성능 지표 계산 및 출력\n","val_preds = model.predict(validation_generator)\n","val_labels = validation_generator.classes\n","\n","# MSE 및 RMSE 계산\n","mse = mean_squared_error(val_labels, np.argmax(val_preds, axis=1))\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(val_labels, np.argmax(val_preds, axis=1))\n","\n","print(f'MSE: {mse}')\n","print(f'RMSE: {rmse}')\n","print(f'MAE: {mae}')\n","\n","# 모델 저장\n","model.save('resnet152_image_classifier_model.keras', save_format='keras')\n","\n","# 모델 불러오기\n","model = load_model('resnet152_image_classifier_model.keras')\n","\n","# 이미지 파일 목록 가져오기\n","file_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith('.jpg') or fname.endswith('.png')]\n","\n","# 데이터프레임 생성\n","test_df = pd.DataFrame({\n","    'filename': file_paths,\n","    'class': ['unknown'] * len(file_paths)  # 임의의 클래스 레이블\n","})\n","\n","# 이미지 데이터 제너레이터 설정\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","\n","# 데이터프레임에서 데이터 로드\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    x_col='filename',\n","    y_col=None,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=1,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# test_generator에 데이터가 있는지 확인\n","if len(test_generator.filenames) == 0:\n","    print(\"Error: No images found in the test directory.\")\n","else:\n","    print(f\"Found {len(test_generator.filenames)} images in the test directory.\")\n","\n","# 테스트 이미지 예측\n","test_preds = model.predict(test_generator)\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","\n","# 클래스 인덱스를 클래스 이름으로 매핑\n","class_indices = {v: k for k, v in train_generator.class_indices.items()}\n","test_pred_class_labels = [class_indices[label] for label in test_pred_labels]\n","\n","# 결과 저장\n","results = pd.DataFrame({\n","    'filename': test_generator.filenames,\n","    'predicted_label': test_pred_class_labels\n","})\n","results.to_csv('classification_results.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wgZFga_psOg","executionInfo":{"status":"ok","timestamp":1716944580509,"user_tz":-540,"elapsed":1461079,"user":{"displayName":"Inho Son","userId":"16864266206823282486"}},"outputId":"ffa5e746-8528-4949-e0d9-359f337ecef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2400 images belonging to 3 classes.\n","Found 600 images belonging to 3 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n","234698864/234698864 [==============================] - 7s 0us/step\n","Epoch 1/10\n","75/75 [==============================] - 196s 992ms/step - loss: 0.2592 - accuracy: 0.8942 - val_loss: 1.7332 - val_accuracy: 0.3333\n","Epoch 2/10\n","75/75 [==============================] - 74s 980ms/step - loss: 0.0463 - accuracy: 0.9875 - val_loss: 1.8670 - val_accuracy: 0.3333\n","Epoch 3/10\n","75/75 [==============================] - 74s 986ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 2.2874 - val_accuracy: 0.3383\n","Epoch 4/10\n","75/75 [==============================] - 73s 980ms/step - loss: 0.0117 - accuracy: 0.9950 - val_loss: 2.9678 - val_accuracy: 0.3333\n","Epoch 5/10\n","75/75 [==============================] - 73s 976ms/step - loss: 0.0466 - accuracy: 0.9833 - val_loss: 1.6819 - val_accuracy: 0.3333\n","Epoch 6/10\n","75/75 [==============================] - 74s 980ms/step - loss: 0.0385 - accuracy: 0.9862 - val_loss: 1.6063 - val_accuracy: 0.3333\n","Epoch 7/10\n","75/75 [==============================] - 73s 978ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 3.0238 - val_accuracy: 0.3333\n","Epoch 8/10\n","75/75 [==============================] - 73s 972ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 2.3965 - val_accuracy: 0.3483\n","Epoch 9/10\n","75/75 [==============================] - 74s 980ms/step - loss: 0.0130 - accuracy: 0.9962 - val_loss: 1.5115 - val_accuracy: 0.5267\n","Epoch 10/10\n","75/75 [==============================] - 74s 979ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 0.5447 - val_accuracy: 0.7750\n","19/19 [==============================] - 9s 270ms/step\n","MSE: 1.4216666666666666\n","RMSE: 1.1923366415013281\n","MAE: 0.915\n","Found 45 validated image filenames.\n","Found 45 images in the test directory.\n","45/45 [==============================] - 5s 23ms/step\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OTMmlAAZvBPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pR_l9mIgZGI8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716945477654,"user_tz":-540,"elapsed":54488,"user":{"displayName":"Inho Son","userId":"16864266206823282486"}},"outputId":"8566bd2b-eb6e-414a-aad8-79c7d2fd4cac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2400 images belonging to 3 classes.\n","Found 600 images belonging to 3 classes.\n","Epoch 1/10\n","75/75 [==============================] - 20s 176ms/step - loss: 1.0924 - accuracy: 0.5833 - val_loss: 0.7377 - val_accuracy: 0.6823\n","Epoch 2/10\n","75/75 [==============================] - 12s 164ms/step - loss: 0.5794 - accuracy: 0.7617 - val_loss: 0.5181 - val_accuracy: 0.7969\n","Epoch 3/10\n","75/75 [==============================] - 13s 170ms/step - loss: 0.4324 - accuracy: 0.8246 - val_loss: 0.5358 - val_accuracy: 0.7743\n","Epoch 4/10\n","75/75 [==============================] - 13s 171ms/step - loss: 0.3241 - accuracy: 0.8775 - val_loss: 0.5040 - val_accuracy: 0.8108\n","Epoch 5/10\n","75/75 [==============================] - 13s 169ms/step - loss: 0.2214 - accuracy: 0.9192 - val_loss: 0.5410 - val_accuracy: 0.8003\n","Epoch 6/10\n","75/75 [==============================] - 12s 163ms/step - loss: 0.1581 - accuracy: 0.9413 - val_loss: 0.7256 - val_accuracy: 0.7743\n","Epoch 7/10\n","75/75 [==============================] - 12s 164ms/step - loss: 0.1164 - accuracy: 0.9588 - val_loss: 0.7804 - val_accuracy: 0.7830\n","Epoch 8/10\n","75/75 [==============================] - 13s 168ms/step - loss: 0.0462 - accuracy: 0.9862 - val_loss: 0.8336 - val_accuracy: 0.7969\n","Epoch 9/10\n","75/75 [==============================] - 12s 165ms/step - loss: 0.0254 - accuracy: 0.9937 - val_loss: 0.9545 - val_accuracy: 0.8073\n","Epoch 10/10\n","75/75 [==============================] - 12s 162ms/step - loss: 0.0439 - accuracy: 0.9829 - val_loss: 1.2157 - val_accuracy: 0.7552\n","19/19 [==============================] - 2s 118ms/step\n","MSE: 1.3983333333333334\n","RMSE: 1.1825114516711175\n","MAE: 0.925\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Found 45 validated image filenames.\n","Found 45 images in the test directory.\n","45/45 [==============================] - 0s 4ms/step\n"]}],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","import pandas as pd\n","\n","# 경로 설정\n","train_dir = './train'\n","test_dir = './test'\n","\n","# 사용자 정의 RMSE 함수 정의\n","def root_mean_squared_error(y_true, y_pred):\n","    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n","\n","# 이미지 데이터 제너레이터 설정\n","train_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)  # 데이터 정규화 및 검증 데이터 분할\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","# 모델 정의\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(128, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'mse', root_mean_squared_error])\n","\n","# 모델 학습\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // validation_generator.batch_size,\n","    epochs=10\n",")\n","\n","# 검증 데이터 예측\n","val_preds = model.predict(validation_generator)\n","val_labels = validation_generator.classes\n","\n","# MSE 및 RMSE 계산\n","mse = mean_squared_error(val_labels, np.argmax(val_preds, axis=1))\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(val_labels, np.argmax(val_preds, axis=1))\n","\n","print(f'MSE: {mse}')\n","print(f'RMSE: {rmse}')\n","print(f'MAE: {mae}')\n","\n","model.save('image_classifier_model.h5')\n","\n","# 모델 불러오기\n","model = load_model('image_classifier_model.h5')\n","\n","# 이미지 파일 목록 가져오기\n","file_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith('.jpg') or fname.endswith('.png')]\n","\n","# 데이터프레임 생성\n","test_df = pd.DataFrame({\n","    'filename': file_paths,\n","    'class': ['unknown'] * len(file_paths)  # 임의의 클래스 레이블\n","})\n","\n","# 이미지 데이터 제너레이터 설정\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","\n","# 데이터프레임에서 데이터 로드\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    x_col='filename',\n","    y_col=None,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=1,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# test_generator에 데이터가 있는지 확인\n","if len(test_generator.filenames) == 0:\n","    print(\"Error: No images found in the test directory.\")\n","else:\n","    print(f\"Found {len(test_generator.filenames)} images in the test directory.\")\n","    # 테스트 이미지 예측\n","    test_preds = model.predict(test_generator)\n","    test_pred_labels = np.argmax(test_preds, axis=1)\n","\n","    # 클래스 인덱스를 클래스 이름으로 변환\n","    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n","    test_pred_class_names = [class_indices[label] for label in test_pred_labels]\n","\n","    # 결과 저장\n","    results = pd.DataFrame({\n","        'filename': test_generator.filenames,\n","        'predicted_label': test_pred_class_names\n","    })\n","    results.to_csv('classification_results.csv', index=False)\n"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSMjNsBPLWbI","executionInfo":{"status":"ok","timestamp":1716933000146,"user_tz":-540,"elapsed":310,"user":{"displayName":"INho SON","userId":"00670742937371649122"}},"outputId":"227d833e-01c5-4385-9b2d-1059791aaf33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","import pandas as pd\n","\n","# 경로 설정\n","train_dir = './train'\n","test_dir = './test'\n","\n","# 이미지 데이터 제너레이터 설정\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255.0,\n","    validation_split=0.2,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")  # 데이터 정규화 및 데이터 증강\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","# 모델 정의\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n","    MaxPooling2D((2, 2)),\n","    Dropout(0.25),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Dropout(0.25),\n","    Conv2D(128, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Dropout(0.25),\n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dropout(0.5),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // validation_generator.batch_size,\n","    epochs=20  # 에포크 수 증가\n",")\n","\n","# 검증 데이터 예측\n","val_preds = model.predict(validation_generator)\n","val_labels = validation_generator.classes\n","\n","# MSE 및 RMSE 계산\n","mse = mean_squared_error(val_labels, np.argmax(val_preds, axis=1))\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(val_labels, np.argmax(val_preds, axis=1))\n","\n","print(f'MSE: {mse}')\n","print(f'RMSE: {rmse}')\n","print(f'MAE: {mae}')\n","\n","model.save('image_classifier_model.h5')\n","\n","# 모델 불러오기\n","model = load_model('image_classifier_model.h5')\n","\n","# 이미지 파일 목록 가져오기\n","file_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith('.jpg') or fname.endswith('.png')]\n","\n","# 데이터프레임 생성\n","test_df = pd.DataFrame({\n","    'filename': file_paths,\n","    'class': ['unknown'] * len(file_paths)  # 임의의 클래스 레이블\n","})\n","\n","# 이미지 데이터 제너레이터 설정\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","\n","# 데이터프레임에서 데이터 로드\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    x_col='filename',\n","    y_col=None,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=1,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# test_generator에 데이터가 있는지 확인\n","if len(test_generator.filenames) == 0:\n","    print(\"Error: No images found in the test directory.\")\n","else:\n","    print(f\"Found {len(test_generator.filenames)} images in the test directory.\")\n","    # 테스트 이미지 예측\n","    test_preds = model.predict(test_generator)\n","    test_pred_labels = np.argmax(test_preds, axis=1)\n","\n","    # 클래스 인덱스를 클래스 이름으로 변환\n","    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n","    test_pred_class_names = [class_indices[label] for label in test_pred_labels]\n","\n","    # 결과 저장\n","    results = pd.DataFrame({\n","        'filename': test_generator.filenames,\n","        'predicted_label': test_pred_class_names\n","    })\n","    results.to_csv('classification_results.csv', index=False)\n"],"metadata":{"id":"UacXRm2YK-Wv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716946866168,"user_tz":-540,"elapsed":1311489,"user":{"displayName":"Inho Son","userId":"16864266206823282486"}},"outputId":"719e386c-4148-427e-f86e-960d9918de70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2400 images belonging to 3 classes.\n","Found 600 images belonging to 3 classes.\n","Epoch 1/20\n","75/75 [==============================] - 52s 662ms/step - loss: 1.5024 - accuracy: 0.4750 - val_loss: 0.8784 - val_accuracy: 0.6128\n","Epoch 2/20\n","75/75 [==============================] - 51s 675ms/step - loss: 0.8917 - accuracy: 0.6346 - val_loss: 0.7772 - val_accuracy: 0.6979\n","Epoch 3/20\n","75/75 [==============================] - 50s 664ms/step - loss: 0.7689 - accuracy: 0.6812 - val_loss: 0.7884 - val_accuracy: 0.7049\n","Epoch 4/20\n","75/75 [==============================] - 53s 703ms/step - loss: 0.7015 - accuracy: 0.7158 - val_loss: 0.7091 - val_accuracy: 0.7413\n","Epoch 5/20\n","75/75 [==============================] - 51s 685ms/step - loss: 0.6867 - accuracy: 0.7179 - val_loss: 0.6618 - val_accuracy: 0.7188\n","Epoch 6/20\n","75/75 [==============================] - 52s 700ms/step - loss: 0.6581 - accuracy: 0.7262 - val_loss: 0.7169 - val_accuracy: 0.7448\n","Epoch 7/20\n","75/75 [==============================] - 51s 681ms/step - loss: 0.6649 - accuracy: 0.7167 - val_loss: 0.6436 - val_accuracy: 0.7865\n","Epoch 8/20\n","75/75 [==============================] - 52s 698ms/step - loss: 0.6293 - accuracy: 0.7358 - val_loss: 0.6260 - val_accuracy: 0.7622\n","Epoch 9/20\n","75/75 [==============================] - 54s 711ms/step - loss: 0.6081 - accuracy: 0.7521 - val_loss: 0.6152 - val_accuracy: 0.7587\n","Epoch 10/20\n","75/75 [==============================] - 52s 700ms/step - loss: 0.6145 - accuracy: 0.7458 - val_loss: 0.5904 - val_accuracy: 0.7795\n","Epoch 11/20\n","75/75 [==============================] - 51s 688ms/step - loss: 0.5959 - accuracy: 0.7517 - val_loss: 0.6035 - val_accuracy: 0.7656\n","Epoch 12/20\n","75/75 [==============================] - 54s 721ms/step - loss: 0.6098 - accuracy: 0.7533 - val_loss: 0.5641 - val_accuracy: 0.7743\n","Epoch 13/20\n","75/75 [==============================] - 51s 677ms/step - loss: 0.5938 - accuracy: 0.7538 - val_loss: 0.6029 - val_accuracy: 0.7656\n","Epoch 14/20\n","75/75 [==============================] - 52s 689ms/step - loss: 0.5959 - accuracy: 0.7517 - val_loss: 0.5773 - val_accuracy: 0.7691\n","Epoch 15/20\n","75/75 [==============================] - 52s 692ms/step - loss: 0.5765 - accuracy: 0.7725 - val_loss: 0.5787 - val_accuracy: 0.7812\n","Epoch 16/20\n","75/75 [==============================] - 52s 692ms/step - loss: 0.5612 - accuracy: 0.7696 - val_loss: 0.5934 - val_accuracy: 0.7656\n","Epoch 17/20\n","75/75 [==============================] - 52s 693ms/step - loss: 0.5724 - accuracy: 0.7696 - val_loss: 0.5524 - val_accuracy: 0.8021\n","Epoch 18/20\n","75/75 [==============================] - 52s 693ms/step - loss: 0.5523 - accuracy: 0.7750 - val_loss: 0.5498 - val_accuracy: 0.8003\n","Epoch 19/20\n","75/75 [==============================] - 53s 711ms/step - loss: 0.5567 - accuracy: 0.7746 - val_loss: 0.5648 - val_accuracy: 0.7847\n","Epoch 20/20\n","75/75 [==============================] - 51s 680ms/step - loss: 0.5417 - accuracy: 0.7867 - val_loss: 0.5625 - val_accuracy: 0.7569\n","19/19 [==============================] - 10s 540ms/step\n","MSE: 1.33\n","RMSE: 1.1532562594670797\n","MAE: 0.8933333333333333\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Found 45 validated image filenames.\n","Found 45 images in the test directory.\n","45/45 [==============================] - 0s 8ms/step\n"]}]},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import ResNet152\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, Callback\n","from sklearn.metrics import mean_absolute_error\n","import numpy as np\n","import pandas as pd\n","\n","# 사용자 정의 콜백 정의\n","class MetricsCallback(Callback):\n","    def __init__(self, validation_data):\n","        super().__init__()\n","        self.validation_data = validation_data\n","        self.val_labels = []\n","        for _, y in validation_data:\n","            self.val_labels.extend(np.argmax(y, axis=1))\n","        self.val_labels = np.array(self.val_labels)\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        val_preds = self.model.predict(self.validation_data)\n","        mae = mean_absolute_error(self.val_labels, np.argmax(val_preds, axis=1))\n","        print(f'Epoch {epoch + 1} - val_mae: {mae:.4f}')\n","\n","# 경로 설정\n","train_dir = './train'\n","test_dir = './test'\n","\n","# 이미지 데이터 제너레이터 설정\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255.0,\n","    validation_split=0.2,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")  # 데이터 정규화 및 데이터 증강\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training',\n","    seed=42\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation',\n","    seed=42\n",")\n","\n","# 프리트레인드 ResNet-152 모델 불러오기\n","base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# 일부 레이어 고정\n","for layer in base_model.layers[:-10]:\n","    layer.trainable = False\n","\n","# 모델 정의\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dropout(0.5),\n","    Dense(512, activation='relu'),\n","    Dropout(0.5),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# 모델 컴파일\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 학습률 감소 콜백\n","lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.00001)\n","\n","# 사용자 정의 콜백 인스턴스 생성\n","metrics_callback = MetricsCallback(validation_data=validation_generator)\n","\n","# 모델 학습\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    validation_data=validation_generator,\n","    validation_steps=len(validation_generator),\n","    epochs=10,\n","    callbacks=[lr_reduction, metrics_callback]\n",")\n","\n","# 성능 지표 계산 및 출력 (학습 후)\n","val_preds = model.predict(validation_generator, steps=len(validation_generator))\n","val_labels = []\n","for _, y in validation_generator:\n","    val_labels.extend(np.argmax(y, axis=1))\n","val_labels = np.array(val_labels)\n","\n","# MAE 계산\n","mae = mean_absolute_error(val_labels, np.argmax(val_preds, axis=1))\n","print(f'Final MAE: {mae}')\n","\n","# 모델 저장\n","model.save('resnet152_image_classifier_model.keras', save_format='keras')\n","\n","# 모델 불러오기\n","model = load_model('resnet152_image_classifier_model.keras')\n","\n","# 이미지 파일 목록 가져오기\n","file_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith('.jpg') or fname.endswith('.png')]\n","\n","# 데이터프레임 생성\n","test_df = pd.DataFrame({\n","    'filename': file_paths,\n","    'class': ['unknown'] * len(file_paths)  # 임의의 클래스 레이블\n","})\n","\n","# 이미지 데이터 제너레이터 설정\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","\n","# 데이터프레임에서 데이터 로드\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    x_col='filename',\n","    y_col=None,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=1,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# test_generator에 데이터가 있는지 확인\n","if len(test_generator.filenames) == 0:\n","    print(\"Error: No images found in the test directory.\")\n","else:\n","    print(f\"Found {len(test_generator.filenames)} images in the test directory.\")\n","\n","# 테스트 이미지 예측\n","test_preds = model.predict(test_generator)\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","\n","# 클래스 인덱스를 클래스 이름으로 매핑\n","class_indices = {v: k for k, v in train_generator.class_indices.items()}\n","test_pred_class_labels = [class_indices[label] for label in test_pred_labels]\n","\n","# 결과 저장\n","results = pd.DataFrame({\n","    'filename': test_generator.filenames,\n","    'predicted_label': test_pred_class_labels\n","})\n","results.to_csv('classification_results.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMv57h3G7gs8","outputId":"88e69ca0-94e1-4fea-c30e-782e725775a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2400 images belonging to 3 classes.\n","Found 600 images belonging to 3 classes.\n"]}]},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","import pandas as pd\n","\n","# 사용자 정의 RMSE 함수 정의\n","def root_mean_squared_error(y_true, y_pred):\n","    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n","\n","# 경로 설정\n","train_dir = './train'\n","test_dir = './test'\n","\n","# 이미지 데이터 제너레이터 설정\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255.0,\n","    validation_split=0.2,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")  # 데이터 정규화 및 데이터 증강\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training',\n","    seed=42  # 동일한 데이터셋을 유지하기 위해 시드 설정\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation',\n","    seed=42  # 동일한 데이터셋을 유지하기 위해 시드 설정\n",")\n","\n","# 프리트레인드 MobileNetV2 모델 불러오기\n","base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# 일부 레이어 고정\n","for layer in base_model.layers[:-10]:\n","    layer.trainable = False\n","\n","# 모델 정의\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dropout(0.5),\n","    Dense(512, activation='relu'),\n","    Dropout(0.5),\n","    Dense(len(train_generator.class_indices), activation='softmax')\n","])\n","\n","# 모델 컴파일\n","model.compile(optimizer=Adam(learning_rate=0.0001),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy', 'mse', root_mean_squared_error])\n","\n","# 학습률 감소 콜백\n","lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.00001)\n","\n","# 모델 학습\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    validation_data=validation_generator,\n","    validation_steps=len(validation_generator),\n","    epochs=20,\n","    callbacks=[lr_reduction]\n",")\n","\n","# 검증 데이터 예측\n","val_preds = model.predict(validation_generator, steps=len(validation_generator))\n","val_labels = validation_generator.classes\n","\n","# MSE 및 RMSE 계산\n","mse = mean_squared_error(val_labels, np.argmax(val_preds, axis=1))\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(val_labels, np.argmax(val_preds, axis=1))\n","\n","print(f'MSE: {mse}')\n","print(f'RMSE: {rmse}')\n","print(f'MAE: {mae}')\n","\n","# 모델 저장\n","model.save('mobilenetv2_image_classifier_model.keras', save_format='keras')\n","\n","# 모델 불러오기\n","model = load_model('mobilenetv2_image_classifier_model.keras', custom_objects={'root_mean_squared_error': root_mean_squared_error})\n","\n","# 이미지 파일 목록 가져오기\n","file_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith('.jpg') or fname.endswith('.png')]\n","\n","# 데이터프레임 생성\n","test_df = pd.DataFrame({\n","    'filename': file_paths,\n","    'class': ['unknown'] * len(file_paths)  # 임의의 클래스 레이블\n","})\n","\n","# 이미지 데이터 제너레이터 설정\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","\n","# 데이터프레임에서 데이터 로드\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    x_col='filename',\n","    y_col=None,\n","    target_size=(224, 224),  # 이미지를 224x224로 리사이즈\n","    batch_size=1,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# test_generator에 데이터가 있는지 확인\n","if len(test_generator.filenames) == 0:\n","    print(\"Error: No images found in the test directory.\")\n","else:\n","    print(f\"Found {len(test_generator.filenames)} images in the test directory.\")\n","\n","# 테스트 이미지 예측\n","test_preds = model.predict(test_generator)\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","\n","# 클래스 인덱스를 클래스 이름으로 매핑\n","class_indices = {v: k for k, v in train_generator.class_indices.items()}\n","test_pred_class_labels = [class_indices[label] for label in test_pred_labels]\n","\n","# 결과 저장\n","results = pd.DataFrame({\n","    'filename': test_generator.filenames,\n","    'predicted_label': test_pred_class_labels\n","})\n","results.to_csv('classification_results.csv', index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"id":"gtZ51J11A88r","executionInfo":{"status":"error","timestamp":1716949587018,"user_tz":-540,"elapsed":176764,"user":{"displayName":"Inho Son","userId":"16864266206823282486"}},"outputId":"dbd14df1-aba0-4950-844c-51acdf40d0c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2400 images belonging to 3 classes.\n","Found 600 images belonging to 3 classes.\n","Epoch 1/20\n","75/75 [==============================] - 56s 651ms/step - loss: 0.9307 - accuracy: 0.5929 - mse: 0.1815 - root_mean_squared_error: 0.4233 - val_loss: 0.5927 - val_accuracy: 0.7500 - val_mse: 0.1163 - val_root_mean_squared_error: 0.3380 - lr: 1.0000e-04\n","Epoch 2/20\n","75/75 [==============================] - 49s 654ms/step - loss: 0.6275 - accuracy: 0.7433 - mse: 0.1218 - root_mean_squared_error: 0.3463 - val_loss: 0.6746 - val_accuracy: 0.7317 - val_mse: 0.1291 - val_root_mean_squared_error: 0.3581 - lr: 1.0000e-04\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-9428acbef719>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m         \"\"\"Resets the state of all the metrics in the model.\n\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"ic0UNHt1G-Rz"},"execution_count":null,"outputs":[]}]}